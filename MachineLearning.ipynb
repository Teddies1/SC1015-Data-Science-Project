{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8fc2fb-53d0-4b3d-ba9f-6abc47465ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f8bbd-3bad-48a5-88a8-b0f7a92a446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "creates training and validation sets and creates a linear regression model '''\n",
    "\n",
    "trainset = pd.read_csv(filepath) #please only load the preprocessed dataframe with encoded variables and outliers removed for the purpose of training\n",
    "\n",
    "featureList = [\"LATE_AIRCRAFT_DELAY\", \"NAS_DELAY\", \"OP_CARRIER\", \"ORIGIN\", \"DEST\", \"CARRIER_DELAY\", \"DEP_DELAY_NEW\"] #this featureList is configurable\n",
    "\n",
    "trainset = pd.DataFrame(trainset[featureList])\n",
    "\n",
    "\n",
    "\n",
    "featureList = [\"LATE_AIRCRAFT_DELAY\", \"NAS_DELAY\", \"OP_CARRIER\", \"ORIGIN\", \"DEST\", \"CARRIER_DELAY\"]\n",
    "def split_data(trainset, featureList, labels):\n",
    "    X, y = trainset[featureList], trainset[labels]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "def create_linear_model(trainset, labels):\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(trainset, labels)\n",
    "    return regressor\n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(trainset, featureList, \"DEP_DELAY_NEW\")\n",
    "\n",
    "model = create_linear_model(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9622e8-7633-43b5-97e2-367c4fb0c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' utility function for creating a Pandas dataframe of the performances of a model based on Mean SquaredError and Explained Variance'''\n",
    "\n",
    " While using performanceMeasure, please note:\n",
    "\n",
    "1. Model parameter is a list consisting of trained models in ascending order.\n",
    "\n",
    "2. X, y, preds are parameters that are lists consisting of training sets for all models in the beginning in ascending order, \n",
    "    followed by testing sets for all models in ascending order.\n",
    "    \n",
    "Note: this function will work for 1 or 2 models provided in the model list. We designed it this way since we dealt with 2 different models in the beginnng, \n",
    "then proceeded to go off with one model only.'''\n",
    "\n",
    "def performanceMeasure(model, X, y, preds):\n",
    "    performance = {}\n",
    "    \n",
    "    for i in range(0, len(model)):\n",
    "        performance[\"Model\" + str(i+1) + \"Training\"] = []\n",
    "        performance[\"Model\" + str(i+1) + \"Testing\"] = []\n",
    "\n",
    "    no = 0\n",
    "    for column in performance.keys():\n",
    "        measures = []\n",
    "        if no == 0:\n",
    "            measures.append(model[0].score(X[0], y[0]))\n",
    "            measures.append(mean_squared_error(y[0], preds[0]))\n",
    "        if no == 1:\n",
    "            measures.append(model[0].score(X[2], y[2]))\n",
    "            measures.append(mean_squared_error(y[2], preds[2]))\n",
    "        if len(model) > 1:\n",
    "            \n",
    "            if no == 2:\n",
    "                measures.append(model[1].score(X[1], y[1]))\n",
    "                measures.append(mean_squared_error(y[1], preds[1]))\n",
    "            if no == 3:\n",
    "                measures.append(model[1].score(X[3], y[3]))\n",
    "                measures.append(mean_squared_error(y[3], preds[3]))\n",
    "        performance[column] = measures\n",
    "        no += 1\n",
    "    \n",
    "    return pd.DataFrame(performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff439fd3-76d9-4c1a-8b5a-8aa2ba5e55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' utility function for creating a SVM model based on the LinearSVR '''\n",
    "def create_svm_model(X, y, C=0.5):\n",
    "    regressor = LinearSVR(epsilon=0, C=C, loss=\"epsilon_insensitive\", verbose=1)\n",
    "    regressor.fit(X, y)\n",
    "    return regressor\n",
    "\n",
    "svm_model1 = create_svm_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a1095-ab1c-46e5-8331-0491410a2165",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' utility function for running GridSearch on a model provided parameters to tune '''\n",
    "\n",
    "def runGridSearch(model, parameters, X, y):\n",
    "    clf = GridSearchCV(model, parameters)\n",
    "    clf.fit(X, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6fe2e-d10b-4072-9fab-6eac72f9c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' utility function for transforming the dtypes of the Pandas dataframe to a uniform type. This will be useful while training the neural network'''\n",
    "\n",
    "def uniformDtypes(dataframe, columns, dtype):\n",
    "    dataframe[columns] = dataframe[columns].astype(dtype)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c44e33-c38b-4297-ac42-f45e0d288c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' this segment converts the Pandas dataframe into a Tensor which is an essential part for the custom training loop in the neural network. '''\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "indices = list(np.random.randint(0, X_train.shape[0], size=19))\n",
    "X_train.drop(indices, axis=0, inplace=True)\n",
    "y_train.drop(indices, axis=0, inplace=True)\n",
    "indices_test = list(np.random.randint(0, 11380, size=80))\n",
    "X_test.drop(indices_test, axis=0, inplace=True)\n",
    "y_test.drop(indices_test, axis=0, inplace=True)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "train_set = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n",
    "\n",
    "train_set = train_set.shuffle(buffer_size=len(X_train)).batch(BATCH_SIZE)\n",
    "test_set = test_set.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b00c236-16e9-4e80-97b2-984fee338215",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' defining the core model architecture. The architecture can be modified as well by changing th feedforward pass and the layer sused '''\n",
    "class DNN(tf.keras.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(DNN, self).__init__()\n",
    "        self.initializer = tf.keras.initializers.GlorotUniform()\n",
    "        self.input_layer = tf.keras.layers.InputLayer(input_shape=input_shape)\n",
    "        self.dense1 = tf.keras.layers.Dense(units=512, activation='relu', kernel_initializer=self.initializer)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=256, activation='relu', kernel_initializer=self.initializer)\n",
    "        self.dense3 = tf.keras.layers.Dense(units=128, activation='relu', kernel_initializer=self.initializer)\n",
    "        self.dense4 = tf.keras.layers.Dense(units=64, activation='relu', kernel_initializer=self.initializer)\n",
    "        self.output_layer = tf.keras.layers.Dense(units=1, activation='linear')\n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        out_val = self.output_layer(x)\n",
    "        \n",
    "        return out_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d908c8c-5bf7-4c80-878c-2a6d813e3b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' this is where we have defined the custom training loop, using graph mode. You can use any optimizer, loss, or metric, however, it should inherit from the tf.keras.Loss\n",
    "tf.keras.Optimizers, and tf.keras.Metrics classes respectively. ''''\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "@tf.function\n",
    "def apply_gradients(optimizer, loss, model, labels, dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(dataset)\n",
    "        loss_val = loss(y_true=labels, y_pred=logits)\n",
    "        \n",
    "    gradients = tape.gradient(loss_val, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients,  model.trainable_weights))\n",
    "        \n",
    "    return logits, loss_val\n",
    "    \n",
    "def train_one_epoch(train, train_acc_metric, optimizer, loss, model):\n",
    "    losses = []\n",
    "    pbar = tqdm(total=len(list(enumerate(train))), position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train):\n",
    "        logits, loss_value = apply_gradients(optimizer, loss, model, y_batch_train, x_batch_train)\n",
    "        losses.append(loss_value)\n",
    "        train_acc_metric(y_batch_train, logits)\n",
    "        pbar.set_description(\"Training loss for step %s: %.4f\" % (int(step), float(loss_value)))\n",
    "        pbar.update()\n",
    "    return losses\n",
    "\n",
    "\n",
    "def perform_validation(test, model, loss, val_acc_metric):\n",
    "    losses = []\n",
    "    for x_val, y_val in test:\n",
    "        val_logits = model(x_val)\n",
    "        val_loss = loss(y_true=y_val, y_pred=val_logits)\n",
    "        losses.append(val_loss)\n",
    "        val_acc_metric(y_val, val_logits)\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def train_n_epochs(train, test, loss, optimizer, model, epochs, train_acc_metric, val_acc_metric):\n",
    "    epoch_train_losses, epoch_val_losses = [], []\n",
    "    history = {}\n",
    "    for epoch in range(epochs):\n",
    "        losses_train = train_one_epoch(train, train_acc_metric, optimizer, loss, model)\n",
    "        train_acc_metric.result()\n",
    "        losses_val = perform_validation(test, model, loss, val_acc_metric)\n",
    "        val_acc_metric.result()\n",
    "        mean_train_loss = np.mean(losses_train)\n",
    "        mean_val_loss = np.mean(losses_val)\n",
    "        epoch_train_losses.append(mean_train_loss)\n",
    "        epoch_val_losses.append(mean_val_loss)\n",
    "        history['epoch' + str(epoch+1) + 'train'] = mean_train_loss\n",
    "        history['epoch' + str(epoch+1) + 'val'] = mean_val_loss\n",
    "        print('\\n Epoch %s: Train loss: %.4f  Validation Loss: %.4f, Train Accuracy: %.4f, Validation Accuracy %.4f' % (epoch, float(mean_train_loss), float(mean_val_loss), float(train_acc_metric.result()), float(val_acc_metric.result())))\n",
    "        train_acc_metric.reset_states()\n",
    "        val_acc_metric.reset_states()\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630257a4-4092-4997-9be1-fbff05f396c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' utility function for plotting the validation and training losses per epoch. Please use history as defined above. '''\n",
    "\n",
    "def plot_metrics(history):\n",
    "    num_epochs = int(len(list(history.keys())) / 2)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_losses.append(history['epoch' + str(epoch+1) + 'train'])\n",
    "        val_losses.append(history['epoch' + str(epoch+1) + 'val'])\n",
    "    \n",
    "    epochs = [epoch for epoch in range(num_epochs)]\n",
    "    plt.plot(epochs, train_losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training_Loss')\n",
    "    plt.plot(epochs, val_losses)\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6325b-168e-4d84-8568-d13b4f156b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' this segment trains a XGBoost model and a utility function for the mean squared error loss function to plot the losses for the xgboost predictions'''\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    return np.mean(np.square(y_true-y_pred))\n",
    "\n",
    "preds_train = model.predict(X_train)\n",
    "preds_test = model.predict(X_test)\n",
    "loss_train = loss_function(y_train, preds_train)\n",
    "loss_test = loss_function(y_test, preds_test)\n",
    "print(f\"The train loss is {loss_train}\")\n",
    "print(f\"The test loss is {loss_test}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c94111-72c7-4ab9-81ba-f580f1dff42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' utility function for plotting the feature importances of the xgboost model '''\n",
    "\n",
    "def feature_importances(model, featureList):\n",
    "    f = plt.figure(figsize=(16, 10))\n",
    "    plt.plot([feature for feature in featureList], model.feature_importances_)\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Feature Importance Scores\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
